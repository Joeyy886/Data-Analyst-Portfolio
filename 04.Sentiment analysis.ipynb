{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b86f878-f80d-4584-bddd-334c13d7bdc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95aba65-debe-47b7-99df-57c1bf48be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARY\n",
    "# For decompressing and processing data\n",
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import logging.handlers\n",
    "\n",
    "# For scraping Reddit submissions\n",
    "!pip install praw\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# For data cleaning and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Import NLTK for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Import the sentiment analysis tool\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Install and load the SpaCy package\n",
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "# Download and install the SpaCy English language model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# For topic modeling\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# For temporal analysis visualization\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# For date formatting in charts\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# For interactive visualization\n",
    "import plotly.express as px\n",
    "\n",
    "import ast\n",
    "\n",
    "# For statistic test \n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830c61b-c95c-4ded-a015-69355d7d5eec",
   "metadata": {},
   "source": [
    "## 4.Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec68b23-e06a-47d2-83a6-09908d754883",
   "metadata": {},
   "source": [
    "This section of the code includes:\n",
    "\n",
    "1. Vader analysis: Analyze Vader sentiment polarity on each post, VADER descriptive statistics and Visualize sentiment changes over time.\n",
    "\n",
    "2. NRC-EIL analysis: Analyze 8 emotions by NRC-EIL on each post, NRC-EIL descriptive statistics and Visualize 8 emotion changes over time.\n",
    "\n",
    "3. Topic sentiment/emotion analysis: VADER Sentiment in topic level by period and NRC-EIL Emotion in topic level by period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4e855-5e90-47cf-9c63-45dcea49a107",
   "metadata": {},
   "source": [
    "### 4.1.Vader analysis\n",
    "\n",
    "- a. Analyze Vader sentiment polarity on each post.\n",
    "- b. VADER descriptive statistics.\n",
    "- c. Visualize sentiment changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521df33a-5a25-4bba-afd4-dea9f41818fa",
   "metadata": {},
   "source": [
    "#### a. Analyze Vader sentiment polarity on each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53111a27-d46f-4f96-bb80-7b0686778aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the submissions for VADER analysis\n",
    "vader_submissions = pd.read_csv('/Users/Desktop/Reddit data/preprocessed_vader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371543fa-3adb-4737-86f3-de7f0a92781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77403e7a-7b4d-42f4-9e80-9bd0675c3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the VADER analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment on each post\n",
    "def analyze_vader_sentiment(text):\n",
    "    text = str(text) # convert text to string type\n",
    "    scores = sid.polarity_scores(text) # get sentiment score\n",
    "    # determine the emotion category based on the comprehensive score\n",
    "    if scores['compound'] > 0.15:\n",
    "        sentiment = 'positive' # if compound scores > 0.15 then positive sentiment\n",
    "    elif scores['compound'] < -0.15: # if compound scores < -0.14 then negative sentiment\n",
    "        sentiment = 'negative'\n",
    "    elif scores['compound'] >= -0.15 and scores['compound'] <= 0.15:\n",
    "        sentiment = 'neutral' # otherwise netural sentiment for -0.15< compound < 0.15\n",
    "    # return the sentiment category and score\n",
    "    return sentiment, scores['pos'], scores['neg'], scores['neu'], scores['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f7272-5ee7-4af9-bedb-52bb7f68bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to each post\n",
    "vader_submissions['Sentiment'], vader_submissions['Positive'], vader_submissions['Negative'], vader_submissions['Neutral'], vader_submissions['Compound_Score'] = zip(*vader_submissions['Selftext'].apply(analyze_vader_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ce0c8-9526-4866-b953-8ed33d82d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a0b4e-6f26-418e-be04-d7191f4a601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_submissions.to_csv('/Users/Desktop/Reddit data/vader_submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aca59a-d05b-4f5c-a8d1-d8500ff66568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Vader analysis to the example post\n",
    "post_text = \"I'm proud because in the past this was always the start of a pattern: If I missed gym once I missed it a second time because the perfect week was ruined anyways. Then I often would not go back to the gym at all. But not this time. I just called my gym buddy and told him to meet two days later at the gym so we could work out together. I found it really easy to stay consistent when there is someone who expects me to show up. But there is more success! My gym buddy who I relied on in the first weeks worked day shifts for 2 weeks straight now, so he could not work out when I did. But I still got my workout done, alone, with my own willpower! I also sticked to my meal plan and lost some pounds because the success in the gym motivated me.\"\n",
    "\n",
    "sentiment, pos_score, neg_score, neu_score, compound_score = analyze_vader_sentiment(post_text)\n",
    "\n",
    "# print the output\n",
    "print(f\"{post_text}\")\n",
    "print()\n",
    "print(f\"Sentiment: {sentiment}, Positive Score: {pos_score}, Negative Score: {neg_score}, Neutral Score: {neu_score}, Compound Score: {compound_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a48c87-3b31-4635-a4b3-80d9c97af1df",
   "metadata": {},
   "source": [
    "#### b. VADER descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d0af0-6578-419c-a2c5-25fd1a973cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of negative, positive, and neutral posts in each time period\n",
    "sentiment_counts = vader_submissions.groupby(['Period', 'Sentiment']).size()#.unstack(fill_value=0)\n",
    "\n",
    "# Filter the data for pre-pandemic and during-pandemic periods\n",
    "pre_pandemic_posts = vader_submissions[vader_submissions['Period'] == 'pre_pandemic']\n",
    "dur_pandemic_posts = vader_submissions[vader_submissions['Period'] == 'dur_pandemic']\n",
    "\n",
    "# Count the number of positive, negative, and neutral posts in each period\n",
    "sentiment_counts_pre = pre_pandemic_posts['Sentiment'].value_counts()\n",
    "sentiment_counts_dur = dur_pandemic_posts['Sentiment'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each sentiment category within each period\n",
    "sentiment_percentages_pre = sentiment_counts_pre / sentiment_counts_pre.sum() * 100\n",
    "sentiment_percentages_dur = sentiment_counts_dur / sentiment_counts_dur.sum() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0e633-a0cb-4b64-bef2-dcef20f5f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dded3a6-bea7-4009-bc4d-cd15070eaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sentiment_percentages_pre, sentiment_percentages_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b9775-f1d0-4139-a6f2-8f0a6143e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-square test\n",
    "data = np.array([[13294, 4572],\n",
    "                 [11455, 5619]])\n",
    "chi2, p, dof, ex = chi2_contingency(data)\n",
    "print(f\"Chi2 Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4d919-89d7-4ffe-85f4-92fc1957469b",
   "metadata": {},
   "source": [
    "### 4.2.NRC-EIL analysis\n",
    "\n",
    "- a. Analyze 8 emotions by NRC-EIL on each post.\n",
    "- b. NRC-EIL descriptive statistics.\n",
    "- c. Visualize emotion changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245125c4-fc95-42a6-b380-45125f597820",
   "metadata": {},
   "source": [
    "#### a. Analyze 8 emotions by NRC-EIL on each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e635e-dd29-492a-a152-4dbab8835e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emotion lexicon and submissions data\n",
    "lex = pd.read_csv('/Users/Desktop/Reddit data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', sep='\\t', names=['word', 'emotion', 'present'])\n",
    "lex = lex[lex.present == 1]\n",
    "\n",
    "nrc_submissions = pd.read_csv('/Users/Desktop/Reddit data/preprocessed_nrc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc45bb-1cde-4193-a2bc-5b46cae5561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22855c95-017e-4faf-984a-dbd3619a2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NRC-EIL sentiment analysis\n",
    "def analyze_nrc_sentiment(text):\n",
    "    sentiment_scores = {emotion: 0 for emotion in lex.emotion.unique()}\n",
    "    for word in text.split():\n",
    "        matches = lex[(lex.word == word) & (lex.emotion != 'positive') & (lex.emotion != 'negative')]  # Exclude positive and negative emotions\n",
    "        for _, match in matches.iterrows():\n",
    "            sentiment_scores[match['emotion']] += 1\n",
    "    return sentiment_scores\n",
    "\n",
    "nrc_submissions['Emotion'] = nrc_submissions['Selftext'].apply(analyze_nre_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28beb76-532e-40f9-b641-e806e33b7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nrc-eil analysis to the example post\n",
    "post_text = \"I'm proud because in the past this was always the start of a pattern: If I missed gym once I missed it a second time because the perfect week was ruined anyways. Then I often would not go back to the gym at all. But not this time. I just called my gym buddy and told him to meet two days later at the gym so we could work out together. I found it really easy to stay consistent when there is someone who expects me to show up. But there is more success! My gym buddy who I relied on in the first weeks worked day shifts for 2 weeks straight now, so he could not work out when I did. But I still got my workout done, alone, with my own willpower! I also sticked to my meal plan and lost some pounds because the success in the gym motivated me.\"\n",
    "sentiment_scores = analyze_nrc_sentiment(post_text)\n",
    "\n",
    "#print th result\n",
    "print(f\"{post_text}\")\n",
    "print()\n",
    "scores_output = \", \".join([f\"{emotion.capitalize()} : {score}\" for emotion, score in sentiment_scores.items() if emotion not in ['positive', 'negative']])\n",
    "print(scores_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed907197-141b-4f41-bb36-653c9f528e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_submissions.to_csv('/Users/Desktop/Reddit data/nrc_submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad66e7a-fad3-47ee-99ec-5096f6d08b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_submissions = pd.read_csv('/Users/Desktop/Reddit data/nrc_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391971c-243a-4593-9a20-ccd6ade4dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely evaluate the string representation of the dictionary\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Apply the safe_literal_eval function to the 'Emotion' column\n",
    "nrc_submissions['Emotion'] = nrc_submissions['Emotion'].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea66865-28e2-46fc-bda2-654e1ecc68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dictionary into columns\n",
    "nrc_emotion = nrc_submissions['Emotion'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a710af2-9fd7-47e6-bafc-18b92bc1c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original 'Emotion' column and add emtions to new columns\n",
    "nrc_submissions = pd.concat([nrc_submissions.drop(['Emotion'], axis=1), nrc_emotion], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d66f8e-b21e-4af8-aebb-9e986ffc9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove positive and negative colmun\n",
    "nrc_submissions['Creation Time'] = pd.to_datetime(nrc_submissions['Creation Time'])\n",
    "nrc_submissions = nrc_submissions.drop(columns=['positive', 'negative'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a68df6-00e8-444e-ba8e-08dd6d52cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698160f-8cb8-4c9d-8e12-942f5f50400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_submissions.to_csv('/Users/Desktop/Reddit data/nrc_submissions_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c5ec0c-0aeb-405f-9ded-bc6040ddf52a",
   "metadata": {},
   "source": [
    "#### b. NRC-EIL descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394ca6a-06b6-4844-9b12-3360a564f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a subset of data before and during the pandemic\n",
    "pre_pandemic_data = nrc_submissions[nrc_submissions['Period'] == 'pre_pandemic']\n",
    "dur_pandemic_data = nrc_submissions[nrc_submissions['Period'] == 'dur_pandemic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ad669-1160-4d08-870b-35ef7cace092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of each emotion before the pandemic\n",
    "pre_pandemic_emotions_sum = pre_pandemic_data[['trust', 'fear', 'sadness', 'anger', 'surprise', 'disgust', 'joy', 'anticipation']].sum()\n",
    "pre_pandemic_emotions_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb084a9-1f12-4de3-b071-184956818534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of each emotion before the pandemic\n",
    "pre_pandemic_emotions_percentage = pre_pandemic_emotions_sum / pre_pandemic_emotions_sum.sum() * 100\n",
    "pre_pandemic_emotions_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55bd92-fa95-4349-8ba4-4182c4a55a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of each emotion during the pandemic\n",
    "dur_pandemic_emotions_sum = dur_pandemic_data[['trust', 'fear', 'sadness', 'anger', 'surprise', 'disgust', 'joy', 'anticipation']].sum()\n",
    "dur_pandemic_emotions_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bbf5e1-5e61-4905-9894-7b3de53e68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of each emotion during the pandemic\n",
    "dur_pandemic_emotions_percentage = dur_pandemic_emotions_sum / dur_pandemic_emotions_sum.sum() * 100\n",
    "dur_pandemic_emotions_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ffa73-0bcd-464e-a46c-86eb466ab069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test\n",
    "data = np.array([[165270, 141028,138341,91326,106091,110334,140532,191040],\n",
    "                 [191707, 169208,163727,105152,124785,130163,163295,223038]])\n",
    "\n",
    "chi2, p, dof, ex = chi2_contingency(data)\n",
    "\n",
    "print(f\"Chi2 Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd844809-b50d-4fd3-ae1a-512c726310a8",
   "metadata": {},
   "source": [
    "### 4.3.Topic Sentiment and emotion analysis\n",
    "\n",
    "- a. Sentiment in topic level by period.\n",
    "- b. Emotion in topic level by period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a54ef2-1504-431c-a214-4ca19ddfd723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the submissions for VADER analysis\n",
    "lda_submissions = pd.read_csv('/Users/Desktop/Reddit data/lda_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd6e1b-0ca1-4f30-8415-d9ee831d7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the submissions for VADER analysis\n",
    "vader_submissions = pd.read_csv('/Users/Desktop/Reddit data/vader_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24766a-afca-46bc-b713-cf90b7fc5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the submissions for NRC-EIL analysis\n",
    "nrc_submissions = pd.read_csv('/Users/Desktop/Reddit data/nrc_submissions_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b09c9-3510-47f7-814d-40be4f7d40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_submissions['Topic'] = lda_submissions['Topic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dfb557-2362-4b7e-9a3f-a1db28c687fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_submissions['Topic'] = lda_submissions['Topic'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dafe5bb-7aa2-4118-b250-6da547982102",
   "metadata": {},
   "source": [
    "#### a. Sentiment in topic level by period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904b13f-655f-4b08-abc1-05004767530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by period\n",
    "pre_pandemic_data = vader_submissions[vader_submissions['Period'] == 'pre_pandemic']\n",
    "dur_pandemic_data = vader_submissions[vader_submissions['Period'] == 'dur_pandemic']\n",
    "\n",
    "# Function to calculate sentiment proportions\n",
    "def calculate_sentiment_proportions(df):\n",
    "    sentiment_counts = df.groupby('Topic')['Sentiment'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    sentiment_proportions = sentiment_counts * 100\n",
    "    return sentiment_proportions\n",
    "\n",
    "# Calculate proportions for each period\n",
    "pre_pandemic_sentiments = calculate_sentiment_proportions(pre_pandemic_data)\n",
    "dur_pandemic_sentiments = calculate_sentiment_proportions(dur_pandemic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666abbc-0366-4ecc-8f18-925e5e9714c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "pre_pandemic_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30cfb5-96e9-4616-b3fe-45ec474d4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_pandemic_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6404f9e-793d-4d1d-ace9-263ddbad7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulize the sentiment polarity by topics\n",
    "# custom the topic orders\n",
    "topics = [2, 11, 9, 1, 10, 12, 3, 14, 4, 15, 5, 6, 13, 8, 7, 0]\n",
    "pre_pandemic_sentiments_ordered = pre_pandemic_sentiments.loc[topics]\n",
    "dur_pandemic_sentiments_ordered = dur_pandemic_sentiments.loc[topics]\n",
    "\n",
    "# Extract topics and sentiment proportions from the dataframes\n",
    "pre_pandemic_positive = pre_pandemic_sentiments_ordered['positive'].tolist()\n",
    "pre_pandemic_negative = pre_pandemic_sentiments_ordered['negative'].tolist()\n",
    "pre_pandemic_neutral = pre_pandemic_sentiments_ordered['neutral'].tolist()\n",
    "\n",
    "dur_pandemic_positive = dur_pandemic_sentiments_ordered['positive'].tolist()\n",
    "dur_pandemic_negative = dur_pandemic_sentiments_ordered['negative'].tolist()\n",
    "dur_pandemic_neutral = dur_pandemic_sentiments_ordered['neutral'].tolist()\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.2  # Width of each bar\n",
    "gap_width = 0.8  # Gap width between topic groups\n",
    "r = np.arange(len(topics)) * (3 * bar_width + gap_width)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot pre-pandemic and during-pandemic positive, negative, and neutral sentiment proportions\n",
    "ax.bar(r - 2.5 * bar_width, pre_pandemic_positive, color='#1f77b4', width=bar_width, label='Pre-pandemic Positive')\n",
    "ax.bar(r - 1.5 * bar_width, dur_pandemic_positive, color='#aec7e8', width=bar_width, label='Dur-pandemic Positive')\n",
    "\n",
    "ax.bar(r - 0.5 * bar_width, pre_pandemic_negative, color='#ff7f0e', width=bar_width, label='Pre-pandemic Negative')\n",
    "ax.bar(r + 0.5 * bar_width, dur_pandemic_negative, color='#ffbb78', width=bar_width, label='Dur-pandemic Negative')\n",
    "\n",
    "ax.bar(r + 1.5 * bar_width, pre_pandemic_neutral, color='#2ca02c', width=bar_width, label='Pre-pandemic Neutral')\n",
    "ax.bar(r + 2.5 * bar_width, dur_pandemic_neutral, color='#98df8a', width=bar_width, label='Dur-pandemic Neutral')\n",
    "\n",
    "# Add labels and title\n",
    "x_labels = ['T2', 'T11', 'T9', 'T1', 'T10', 'T12', 'T3', 'T14', 'T4', 'T15', 'T5', 'T6', 'T13', 'T8', 'T7', 'T0'] # rename x label name\n",
    "ax.set_xticks(r)\n",
    "ax.set_xticklabels(x_labels, fontsize=13)\n",
    "ax.set_ylabel('Percentage (%)', fontsize=15)\n",
    "\n",
    "# Add legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "ax.legend(by_label.values(), by_label.keys(), loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a3266-dddf-40f8-827a-0e5fc47aa4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of posts before and during the pandemic\n",
    "n_pre_pandemic = 16242  # pre-pandemic posts\n",
    "n_dur_pandemic = 19139  # dur-pandemic posts\n",
    "\n",
    "results = [] # create en empty list to store the result\n",
    "\n",
    "# Loop to calculate the positive and negative sentiment proportions for each topic\n",
    "for topic in range(pre_pandemic_sentiments.shape[0]):\n",
    "    # calulate the positive and negative proportion before the pandemic\n",
    "    pre_positive_prop = pre_pandemic_sentiments.loc[topic, 'positive'] / 100\n",
    "    pre_negative_prop = pre_pandemic_sentiments.loc[topic, 'negative'] / 100\n",
    "    \n",
    "    # calulate the positive and negative proportion during the pandemic\n",
    "    dur_positive_prop = dur_pandemic_sentiments.loc[topic, 'positive'] / 100\n",
    "    dur_negative_prop = dur_pandemic_sentiments.loc[topic, 'negative'] / 100\n",
    "    \n",
    "    # calulate the number of successes\n",
    "    count_positive = [pre_positive_prop * n_pre_pandemic, dur_positive_prop * n_dur_pandemic]\n",
    "    count_negative = [pre_negative_prop * n_pre_pandemic, dur_negative_prop * n_dur_pandemic]\n",
    "    \n",
    "    # the total posts in each period\n",
    "    nobs = [n_pre_pandemic, n_dur_pandemic]\n",
    "    \n",
    "    # peform the Z-test\n",
    "    z_stat_positive, p_value_positive = proportions_ztest(count_positive, nobs)\n",
    "    z_stat_negative, p_value_negative = proportions_ztest(count_negative, nobs)\n",
    "    \n",
    "    # save the result\n",
    "    results.append({\n",
    "        'Topic': topic,\n",
    "        'Positive Z-statistic': z_stat_positive,\n",
    "        'Positive P-value': p_value_positive,\n",
    "        'Negative Z-statistic': z_stat_negative,\n",
    "        'Negative P-value': p_value_negative\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0aa63-de76-4a49-a3b3-d992d9d59282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the z-test result\n",
    "for result in results:\n",
    "    print(f\"Topic {result['Topic']}:\")\n",
    "    print(f\"  Positive: Z-Statistic = {result['Positive Z-statistic']:.4f}, P-Value = {result['Positive P-value']:.4f}\")\n",
    "    print(f\"  Negative: Z-Statistic = {result['Negative Z-statistic']:.4f}, P-Value = {result['Negative P-value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f66f47-cfde-4d01-a336-3056569f4922",
   "metadata": {},
   "source": [
    "#### b. Emotion in topic level by period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0650eb-5238-47cb-a495-43bdfa58471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate data by Period and Topic, summing the emotion categories\n",
    "agg_nrc = nrc_submissions.groupby(['Period', 'Topic']).agg({\n",
    "    'joy': 'sum',\n",
    "    'trust': 'sum',\n",
    "    'anticipation': 'sum',\n",
    "    'surprise': 'sum',\n",
    "    'fear': 'sum',\n",
    "    'sadness': 'sum',\n",
    "    'disgust': 'sum',\n",
    "    'anger': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Step 2: Calculate the total number of emotions per topic to use for proportion calculations\n",
    "agg_nrc['total_emotions'] = agg_nrc[['joy', 'trust', 'anticipation', 'surprise', 'fear', 'sadness', 'disgust', 'anger']].sum(axis=1)\n",
    "\n",
    "# Step 3: Calculate proportions for each emotion so that each topic sums to 100%\n",
    "for emotion in ['joy', 'trust', 'anticipation', 'surprise', 'fear', 'sadness', 'disgust', 'anger']:\n",
    "    agg_nrc[emotion + '_prop'] = (agg_nrc[emotion] / agg_nrc['total_emotions']) * 100\n",
    "\n",
    "# Step 4: Split the data into two separate tables\n",
    "pre_pandemic_nrc = agg_nrc[agg_nrc['Period'] == 'pre_pandemic']\n",
    "dur_pandemic_nrc = agg_nrc[agg_nrc['Period'] == 'dur_pandemic']\n",
    "\n",
    "# Step 5: Select the final columns for display, including both raw counts and proportions\n",
    "emotion_columns = ['joy', 'trust', 'anticipation', 'surprise', 'fear', 'sadness', 'disgust', 'anger']\n",
    "pre_pandemic_nrc = pre_pandemic_nrc[['Topic'] + emotion_columns + \n",
    "                                       [emotion + '_prop' for emotion in emotion_columns]]\n",
    "\n",
    "dur_pandemic_nrc = dur_pandemic_nrc[['Topic'] + emotion_columns + \n",
    "                                       [emotion + '_prop' for emotion in emotion_columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38000803-fee2-47b7-8a16-077a7e4cc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the pre-pandemic tables\n",
    "pre_pandemic_nrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2116167-e7b3-4a85-a85d-98f64515d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dur-pandemic tables\n",
    "dur_pandemic_nrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0158d53-ade3-46de-b165-6d90527ea643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the emotion distribution for each period\n",
    "# custom the topic order according the themes\n",
    "topics = [2, 11, 9, 1, 10, 12, 3, 14, 4, 15, 5, 6, 13, 8, 7, 0]\n",
    "x_labels = ['T2', 'T11', 'T9', 'T1', 'T10', 'T12', 'T3', 'T14', 'T4', 'T15', 'T5', 'T6', 'T13', 'T8', 'T7', 'T0']\n",
    "pre_pandemic_sorted = pre_pandemic_nrc.set_index('Topic').loc[topics].reset_index()\n",
    "dur_pandemic_sorted = dur_pandemic_nrc.set_index('Topic').loc[topics].reset_index()\n",
    "\n",
    "# extract eight emotions before the pandemic\n",
    "pre_pandemic = {\n",
    "    'joy': pre_pandemic_sorted['joy_prop'].tolist(),\n",
    "    'trust': pre_pandemic_sorted['trust_prop'].tolist(),\n",
    "    'anticipation': pre_pandemic_sorted['anticipation_prop'].tolist(),\n",
    "    'surprise': pre_pandemic_sorted['surprise_prop'].tolist(),\n",
    "    'fear': pre_pandemic_sorted['fear_prop'].tolist(),\n",
    "    'sadness': pre_pandemic_sorted['sadness_prop'].tolist(),\n",
    "    'disgust': pre_pandemic_sorted['disgust_prop'].tolist(),\n",
    "    'anger': pre_pandemic_sorted['anger_prop'].tolist()\n",
    "}\n",
    "\n",
    "# extract eight emotions during the pandemic\n",
    "dur_pandemic = {\n",
    "    'joy': dur_pandemic_sorted['joy_prop'].tolist(),\n",
    "    'trust': dur_pandemic_sorted['trust_prop'].tolist(),\n",
    "    'anticipation': dur_pandemic_sorted['anticipation_prop'].tolist(),\n",
    "    'surprise': dur_pandemic_sorted['surprise_prop'].tolist(),\n",
    "    'fear': dur_pandemic_sorted['fear_prop'].tolist(),\n",
    "    'sadness': dur_pandemic_sorted['sadness_prop'].tolist(),\n",
    "    'disgust': dur_pandemic_sorted['disgust_prop'].tolist(),\n",
    "    'anger': dur_pandemic_sorted['anger_prop'].tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92ebc4-b451-4ec4-a3bf-c07d25e58d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_distribution(data, topics, x_labels):\n",
    "    bar_width = 0.5  # set the width of the bar\n",
    "    r = np.arange(len(topics))\n",
    "\n",
    "    # set the color to orange (postive emotions) and blue (negative emotions)\n",
    "    positive_colors = ['#ffcc99', '#ff9933', '#ff7f0e', '#ff6f00'] \n",
    "    negative_colors = ['#aec7e8', '#6699cc', '#1f77b4', '#0f5292'] \n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    bottoms = np.zeros(len(r))\n",
    "\n",
    "    # plot the emotion stacked chart\n",
    "    for i, (emotion, values) in enumerate(data.items()):\n",
    "        bars = ax.bar(r, values, bottom=bottoms, width=bar_width, label=emotion.capitalize(), color=positive_colors[i] if i < 4 else negative_colors[i-4])\n",
    "        # add the proportion text into bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_y() + bar.get_height() / 2, f'{value:.1f}%', ha='center', va='center', fontsize=8)\n",
    "        bottoms += np.array(values)\n",
    "    \n",
    "    # add labels and title\n",
    "    ax.set_xticks(r)\n",
    "    ax.set_xticklabels(x_labels, fontsize=13)\n",
    "    ax.set_ylabel('Proportion (%)', fontsize=15)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496e737-c628-47e0-80e2-3e2359ddd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the emotions distribution before the pandemic\n",
    "plot_emotion_distribution(pre_pandemic, topics, x_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339d3a5-1ccd-41af-b805-b29e75b6bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the emotions distribution during the pandemic\n",
    "plot_emotion_distribution(dur_pandemic, topics, x_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
