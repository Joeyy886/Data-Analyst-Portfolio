{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b86f878-f80d-4584-bddd-334c13d7bdc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95aba65-debe-47b7-99df-57c1bf48be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARY\n",
    "# For decompressing and processing data\n",
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import logging.handlers\n",
    "\n",
    "# For scraping Reddit submissions\n",
    "!pip install praw\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# For data cleaning and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Import NLTK for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Import the sentiment analysis tool\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Install and load the SpaCy package\n",
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "# Download and install the SpaCy English language model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# For topic modeling\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# For temporal analysis visualization\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# For date formatting in charts\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# For interactive visualization\n",
    "import plotly.express as px\n",
    "\n",
    "import ast\n",
    "\n",
    "# For statistic test \n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95477e23-1323-4442-8d95-7c1d4411c02f",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41fd2f-5e6f-4c1d-a7cb-592610500839",
   "metadata": {},
   "source": [
    "This section of the code includes:\n",
    "\n",
    "- a. Extract 'r/loseit' submissions IDs from Reddit historical data.\n",
    "- b. Scrape posts and metadata from Reddit.\n",
    "- c. Filter out any removed or deleted posts.\n",
    "- d. Basic statistics overview and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663ea34-d556-4c43-a9ef-7f6e2ff6c606",
   "metadata": {},
   "source": [
    "### a. Extract 'r/loseit' submissions IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072833e-0e20-4a69-9402-320c41b7b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of the code is adapted from an external script available at: \n",
    "# https://github.com/Watchful1/PushshiftDumps/blob/master/scripts/filter_file.py\n",
    "\n",
    "# 1.Define file paths and data extraction parameters\n",
    "\n",
    "# path to the input/output file\n",
    "input_file = r\"/Users/Desktop/Reddit data/loseit_submissions.zst\"\n",
    "output_file = r\"/Users/Desktop/Reddit data/loseit_submissions_2019_2021_ids\"\n",
    "\n",
    "# date range for filtering the data\n",
    "from_date = datetime.strptime(\"2019-03-01\", \"%Y-%m-%d\") # start date\n",
    "to_date = datetime.strptime(\"2021-04-01\", \"%Y-%m-%d\") # end date\n",
    "\n",
    "# specify the output file format\n",
    "output_format = 'txt' \n",
    "\n",
    "# filter data by subreddit\n",
    "field, values = \"subreddit\", [\"loseit\"]\n",
    "\n",
    "# define the data field to be extracted\n",
    "single_field = 'id'\n",
    "\n",
    "# enable logging of errors during data processing\n",
    "write_bad_lines = True\n",
    "\n",
    "# File containing additional filter values\n",
    "values_file = None\n",
    "\n",
    "# control matching strictness in data filtering\n",
    "exact_match = False\n",
    "\n",
    "# 2.Logging setup to capture errors during processing\n",
    "\n",
    "log = logging.getLogger(\"bot\")\n",
    "log.setLevel(logging.INFO)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s')\n",
    "log_str_handler = logging.StreamHandler()\n",
    "log_str_handler.setFormatter(log_formatter)\n",
    "log.addHandler(log_str_handler)\n",
    "if not os.path.exists(\"logs\"):\n",
    "\tos.makedirs(\"logs\")\n",
    "log_file_handler = logging.handlers.RotatingFileHandler(os.path.join(\"logs\", \"bot.log\"), maxBytes=1024*1024*16, backupCount=5)\n",
    "log_file_handler.setFormatter(log_formatter)\n",
    "log.addHandler(log_file_handler)\n",
    "\n",
    "# 3.Function definitions to handle data extraction and processing\n",
    "\n",
    "def write_line_zst(handle, line):\n",
    "\thandle.write(line.encode('utf-8'))\n",
    "\thandle.write(\"\\n\".encode('utf-8'))\n",
    "\n",
    "\n",
    "def write_line_json(handle, obj):\n",
    "\thandle.write(json.dumps(obj))\n",
    "\thandle.write(\"\\n\")\n",
    "\n",
    "\n",
    "def write_line_single(handle, obj, field):\n",
    "\tif field in obj:\n",
    "\t\thandle.write(obj[field])\n",
    "\telse:\n",
    "\t\tlog.info(f\"{field} not in object {obj['id']}\")\n",
    "\thandle.write(\"\\n\")\n",
    "\n",
    "\n",
    "def write_line_csv(writer, obj, is_submission):\n",
    "\toutput_list = []\n",
    "\toutput_list.append(str(obj['score']))\n",
    "\toutput_list.append(datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d\"))\n",
    "\tif is_submission:\n",
    "\t\toutput_list.append(obj['title'])\n",
    "\toutput_list.append(f\"u/{obj['author']}\")\n",
    "\toutput_list.append(f\"https://www.reddit.com{obj['permalink']}\")\n",
    "\tif is_submission:\n",
    "\t\tif obj['is_self']:\n",
    "\t\t\tif 'selftext' in obj:\n",
    "\t\t\t\toutput_list.append(obj['selftext'])\n",
    "\t\t\telse:\n",
    "\t\t\t\toutput_list.append(\"\")\n",
    "\t\telse:\n",
    "\t\t\toutput_list.append(obj['url'])\n",
    "\telse:\n",
    "\t\toutput_list.append(obj['body'])\n",
    "\twriter.writerow(output_list)\n",
    "\n",
    "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
    "\tchunk = reader.read(chunk_size)\n",
    "\tbytes_read += chunk_size\n",
    "\tif previous_chunk is not None:\n",
    "\t\tchunk = previous_chunk + chunk\n",
    "\ttry:\n",
    "\t\treturn chunk.decode()\n",
    "\texcept UnicodeDecodeError:\n",
    "\t\tif bytes_read > max_window_size:\n",
    "\t\t\traise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
    "\t\tlog.info(f\"Decoding error with {bytes_read:,} bytes, reading another chunk\")\n",
    "\t\treturn read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
    "\n",
    "\n",
    "def read_lines_zst(file_name):\n",
    "\twith open(file_name, 'rb') as file_handle:\n",
    "\t\tbuffer = ''\n",
    "\t\treader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "\t\twhile True:\n",
    "\t\t\tchunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
    "\n",
    "\t\t\tif not chunk:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tlines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "\t\t\tfor line in lines[:-1]:\n",
    "\t\t\t\tyield line.strip(), file_handle.tell()\n",
    "\n",
    "\t\t\tbuffer = lines[-1]\n",
    "\n",
    "\t\treader.close()\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file, output_format, field, values, from_date, to_date, single_field, exact_match):\n",
    "\toutput_path = f\"{output_file}.{output_format}\"\n",
    "\tis_submission = \"submission\" in input_file\n",
    "\tlog.info(f\"Input: {input_file} : Output: {output_path} : Is submission {is_submission}\")\n",
    "\twriter = None\n",
    "\tif output_format == \"zst\":\n",
    "\t\thandle = zstandard.ZstdCompressor().stream_writer(open(output_path, 'wb'))\n",
    "\telif output_format == \"txt\":\n",
    "\t\thandle = open(output_path, 'w', encoding='UTF-8')\n",
    "\telif output_format == \"csv\":\n",
    "\t\thandle = open(output_path, 'w', encoding='UTF-8', newline='')\n",
    "\t\twriter = csv.writer(handle)\n",
    "\telse:\n",
    "\t\tlog.error(f\"Unsupported output format {output_format}\")\n",
    "\t\tsys.exit()\n",
    "\n",
    "\tfile_size = os.stat(input_file).st_size\n",
    "\tcreated = None\n",
    "\tmatched_lines = 0\n",
    "\tbad_lines = 0\n",
    "\ttotal_lines = 0\n",
    "\tfor line, file_bytes_processed in read_lines_zst(input_file):\n",
    "\t\ttotal_lines += 1\n",
    "\t\tif total_lines % 100000 == 0:\n",
    "\t\t\tlog.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {total_lines:,} : {matched_lines:,} : {bad_lines:,} : {file_bytes_processed:,}:{(file_bytes_processed / file_size) * 100:.0f}%\")\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tobj = json.loads(line)\n",
    "\t\t\tcreated = datetime.utcfromtimestamp(int(obj['created_utc']))\n",
    "\n",
    "\t\t\tif created < from_date:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif created > to_date:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif field is not None:\n",
    "\t\t\t\tfield_value = obj[field].lower()\n",
    "\t\t\t\tmatched = False\n",
    "\t\t\t\tfor value in values:\n",
    "\t\t\t\t\tif exact_match:\n",
    "\t\t\t\t\t\tif value == field_value:\n",
    "\t\t\t\t\t\t\tmatched = True\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif value in field_value:\n",
    "\t\t\t\t\t\t\tmatched = True\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif not matched:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tmatched_lines += 1\n",
    "\t\t\tif output_format == \"zst\":\n",
    "\t\t\t\twrite_line_zst(handle, line)\n",
    "\t\t\telif output_format == \"csv\":\n",
    "\t\t\t\twrite_line_csv(writer, obj, is_submission)\n",
    "\t\t\telif output_format == \"txt\":\n",
    "\t\t\t\tif single_field is not None:\n",
    "\t\t\t\t\twrite_line_single(handle, obj, single_field)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twrite_line_json(handle, obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlog.info(f\"Something went wrong, invalid output format {output_format}\")\n",
    "\t\texcept (KeyError, json.JSONDecodeError) as err:\n",
    "\t\t\tbad_lines += 1\n",
    "\t\t\tif write_bad_lines:\n",
    "\t\t\t\tif isinstance(err, KeyError):\n",
    "\t\t\t\t\tlog.warning(f\"Key {field} is not in the object: {err}\")\n",
    "\t\t\t\telif isinstance(err, json.JSONDecodeError):\n",
    "\t\t\t\t\tlog.warning(f\"Line decoding failed: {err}\")\n",
    "\t\t\t\tlog.warning(line)\n",
    "\n",
    "\thandle.close()\n",
    "\tlog.info(f\"Complete : {total_lines:,} : {matched_lines:,} : {bad_lines:,}\")\n",
    "\n",
    "# Main function to orchestrate data processing based on user-defined settings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tif single_field is not None:\n",
    "\t\tlog.info(\"Single field output mode, changing output file format to txt\")\n",
    "\t\toutput_format = \"txt\"\n",
    "\n",
    "\tif values_file is not None:\n",
    "\t\tvalues = []\n",
    "\t\twith open(values_file, 'r') as values_handle:\n",
    "\t\t\tfor value in values_handle:\n",
    "\t\t\t\tvalues.append(value.strip().lower())\n",
    "\t\tlog.info(f\"Loaded {len(values)} from values file {values_file}\")\n",
    "\telse:\n",
    "\t\tvalues = [value.lower() for value in values]  # convert to lowercase\n",
    "\n",
    "\tlog.info(f\"Filtering field: {field}\")\n",
    "\tif len(values) <= 20:\n",
    "\t\tlog.info(f\"On values: {','.join(values)}\")\n",
    "\telse:\n",
    "\t\tlog.info(f\"On values:\")\n",
    "\t\tfor value in values:\n",
    "\t\t\tlog.info(value)\n",
    "\tlog.info(f\"Exact match {('on' if exact_match else 'off')}. Single field {single_field}.\")\n",
    "\tlog.info(f\"From date {from_date.strftime('%Y-%m-%d')} to date {to_date.strftime('%Y-%m-%d')}\")\n",
    "\tlog.info(f\"Output format set to {output_format}\")\n",
    "\n",
    "\tinput_files = []\n",
    "\tif os.path.isdir(input_file):\n",
    "\t\tif not os.path.exists(output_file):\n",
    "\t\t\tos.makedirs(output_file)\n",
    "\t\tfor file in os.listdir(input_file):\n",
    "\t\t\tif not os.path.isdir(file) and file.endswith(\".zst\"):\n",
    "\t\t\t\tinput_name = os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n",
    "\t\t\t\tinput_files.append((os.path.join(input_file, file), os.path.join(output_file, input_name)))\n",
    "\telse:\n",
    "\t\tinput_files.append((input_file, output_file))\n",
    "\tlog.info(f\"Processing {len(input_files)} files\")\n",
    "\tfor file_in, file_out in input_files:\n",
    "\t\tprocess_file(file_in, file_out, output_format, field, values, from_date, to_date, single_field, exact_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed2ca7-86e9-4e1f-8eb9-17e7b5f13ca6",
   "metadata": {},
   "source": [
    "### b. Scrape posts and metadata from Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae51a8-48e6-4ce7-91af-1bb88b531622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'submission ids' file\n",
    "submission_ids = pd.read_csv('/Users/Desktop/Reddit data/loseit_submissions_2019_2021_ids.txt', header=None, names=['sub_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c7ac5-8e12-4eac-9478-e0d7da4498aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission_ids) # 115123 ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b778614-32e9-4b10-8cd1-da96573e98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Reddit API access with PRAW\n",
    "reddit = praw.Reddit(client_id='F2PirR2UnatbMaFiVlJlIg',\n",
    "                     client_secret='hcMIMgNKI4vvatkV4Qsc2SjLuDKyhA',\n",
    "                     user_agent='web:*****:v1.0 (by /u/*****)')\n",
    "\n",
    "subreddit = reddit.subreddit('loseit') # set the subreddit\n",
    "batch_size = 10000 # set 10000 submissions per batch\n",
    "submissions = [] # create an empty list to store submissions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cc541-27c4-4a94-9b0d-3d614a8cc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to scrape the submissions through Reddit API\n",
    "\n",
    "for i in range(0, len(submission_ids), batch_size):\n",
    "    batch_ids = submission_ids[i:i+batch_size]  # retrieve current batch of submissions IDs\n",
    "    for sub_id in batch_ids['sub_id']:\n",
    "        try:\n",
    "            # fetch the submissions data from Reddit\n",
    "            submission = reddit.submission(id=sub_id)\n",
    "            submission.comments.replace_more(limit=0) # remove comments beyond the view limit\n",
    "            created_time = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # append the submission details to the list\n",
    "            submissions.append({\n",
    "                'Creation Time': created_time,\n",
    "                'Title': submission.title,\n",
    "                'User': str(submission.author),\n",
    "                'Selftext': submission.selftext,\n",
    "                'Score': submission.score,\n",
    "                'Number of Comments': submission.num_comments,\n",
    "                'URL': submission.url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post {sub_id}: {e}\")\n",
    "\n",
    "        time.sleep(1) # avoid exceeding the rate limits\n",
    "\n",
    "    # convert the current batch of submissions into a dataFrame and save to CSV file\n",
    "    batch_df = pd.DataFrame(submissions)\n",
    "    batch_df.to_csv(f'/Users/Desktop/Reddit data/submissions_2019_2021/submissions_batch_{i}.csv', index=False)\n",
    "\n",
    "# after the process finished, merge and save into a single dataframe\n",
    "submissions_df = pd.DataFrame(submissions)\n",
    "submissions_df.to_csv('/Users/Desktop/Reddit data/submissions.csv', index=False)\n",
    "\n",
    "print(\"All the submissions are finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf3cf3-e379-4171-9bfe-b0ad56324111",
   "metadata": {},
   "source": [
    "### c. Filter out any removed or deleted posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4af5b-e4a6-4c3e-8acb-e815a4c7d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the 'r/loseit' submissions dataset\n",
    "submissions = pd.read_csv('/Users/Desktop/Reddit data/submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757e7c1-e456-40e5-828e-58d12081716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the deleted submissions\n",
    "submissions['Selftext'].str.contains('\\[deleted\\]', na=False).sum() # 21515 submissions are deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd19b40-8491-4abe-9ce6-2d73ec002a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the removed submissions\n",
    "submissions['Selftext'].str.contains('\\[removed\\]', na=False).sum() # 52755 submissions are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ed46d-2fc3-4d6d-a88e-889264b8304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the deleted and removed submissions to NA and remove them\n",
    "submissions['Selftext'].replace(['[deleted]', '[removed]'], np.nan, inplace=True)\n",
    "submissions.dropna(subset=['Selftext'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103af58-c4d0-41a6-810b-7efd94102fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove posts from automoderator\n",
    "submissions = submissions[submissions['Author'] != 'AutoModerator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df3857-7161-4e66-ae29-f728420f4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove posts from official challenges\n",
    "submissions = submissions[~submissions['Title'].str.contains(r'\\[Challenge\\]', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d61f27-441c-4618-813c-c5e98f7881a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of submissions\n",
    "submissions['Selftext'].count() # 35384 submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ee99a-9b9d-4a48-b0a6-b6bd930b8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "submissions = submissions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f2c2c-55c7-4db4-bd7c-2786c288131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_submissions = submissions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ac94e-81e3-4c05-bf9e-8312306a73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Creation Time' column to datetime type\n",
    "cleaned_submissions['Creation Time'] = pd.to_datetime(cleaned_submissions['Creation Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9e015-929c-4164-abb9-9829258f9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column 'Period' to pre/dur pandemic\n",
    "cleaned_submissions['Period'] = np.where(\n",
    "    cleaned_submissions['Creation Time'] < pd.Timestamp('2020-03-11'),\n",
    "    'pre_pandemic',\n",
    "    'dur_pandemic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323f70c-4e85-4ee1-ba2d-da60b0f84285",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c697bb-a06f-46b3-9745-8103f4aefa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned submissions to CSV file\n",
    "cleaned_submissions.to_csv('/Users/Desktop/Reddit data/cleaned_submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c37e76-e4ae-4f1a-a144-5013b23d28d7",
   "metadata": {},
   "source": [
    "### d. Basic statistics overview and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cbfe2-1693-490d-a5a5-12a0aa08be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pandemic = cleaned_submissions[cleaned_submissions['Period'] == 'pre_pandemic']\n",
    "len(cleaned_submissions[cleaned_submissions['Period'] == 'pre_pandemic']) # 16244 posts before the pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aecf9f2-8fba-4d1e-9beb-f4c33bc76380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_pandemic = cleaned_submissions[cleaned_submissions['Period'] == 'dur_pandemic']\n",
    "len(cleaned_submissions[cleaned_submissions['Period'] == 'dur_pandemic']) # 19140 posts after the pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe9971-8bea-476a-8456-8b5095aa3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistics overview of the loseit dataset before the pandemic\n",
    "\n",
    "# Mean\n",
    "pre_pandemic_mean_posts = pre_pandemic['Creation Time'].dt.date.value_counts().mean()\n",
    "pre_pandemic_mean_scores = pre_pandemic['Score'].mean()\n",
    "pre_pandemic.loc[:, 'Words per post'] = pre_pandemic['Selftext'].str.split().apply(len)\n",
    "pre_pandemic_mean_words = pre_pandemic['Words per post'].mean()\n",
    "\n",
    "# Median\n",
    "pre_pandemic_median_posts = pre_pandemic['Creation Time'].dt.date.value_counts().median()\n",
    "pre_pandemic_median_scores = pre_pandemic['Score'].median()\n",
    "pre_pandemic_median_words = pre_pandemic['Words per post'].median()\n",
    "\n",
    "# SD\n",
    "pre_pandemic_sd_posts = pre_pandemic['Creation Time'].dt.date.value_counts().std()\n",
    "pre_pandemic_sd_scores = pre_pandemic['Score'].std()\n",
    "pre_pandemic_sd_words = pre_pandemic['Words per post'].std()\n",
    "\n",
    "# Print the output\n",
    "print(\"Pre-pandemic era:\")\n",
    "print(f\"Posts per day - Mean: {pre_pandemic_mean_posts}, Median: {pre_pandemic_median_posts}, SD: {pre_pandemic_sd_posts}\")\n",
    "print(f\"Scores per post - Mean: {pre_pandemic_mean_scores}, Median: {pre_pandemic_median_scores}, SD: {pre_pandemic_sd_scores}\")\n",
    "print(f\"Words per post - Mean: {pre_pandemic_mean_words}, Median: {pre_pandemic_median_words}, SD: {pre_pandemic_sd_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69289d50-7ab9-4b0b-9a37-f38cbfd4755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistics overview of the loseit dataset during the pandemic\n",
    "\n",
    "# Mean\n",
    "dur_pandemic_mean_posts = dur_pandemic['Creation Time'].dt.date.value_counts().mean()\n",
    "dur_pandemic_mean_scores = dur_pandemic['Score'].mean()\n",
    "dur_pandemic.loc[:, 'Words per post'] = dur_pandemic['Selftext'].str.split().apply(len)\n",
    "dur_pandemic_mean_words = dur_pandemic['Words per post'].mean()\n",
    "\n",
    "# Median\n",
    "dur_pandemic_median_posts = dur_pandemic['Creation Time'].dt.date.value_counts().median()\n",
    "dur_pandemic_median_scores = dur_pandemic['Score'].median()\n",
    "dur_pandemic_median_words = dur_pandemic['Words per post'].median()\n",
    "\n",
    "# SD\n",
    "dur_pandemic_sd_posts = dur_pandemic['Creation Time'].dt.date.value_counts().std()\n",
    "dur_pandemic_sd_scores = dur_pandemic['Score'].std()\n",
    "dur_pandemic_sd_words = dur_pandemic['Words per post'].std()\n",
    "\n",
    "# Print the output\n",
    "print(\"Dur-pandemic era:\")\n",
    "print(f\"Posts per day - Mean: {dur_pandemic_mean_posts}, Median: {dur_pandemic_median_posts}, SD: {dur_pandemic_sd_posts}\")\n",
    "print(f\"Scores per post - Mean: {dur_pandemic_mean_scores}, Median: {dur_pandemic_median_scores}, SD: {dur_pandemic_sd_scores}\")\n",
    "print(f\"Words per post - Mean: {dur_pandemic_mean_words}, Median: {dur_pandemic_median_words}, SD: {dur_pandemic_sd_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819be07e-17c3-4bf8-917e-efef636a01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the submissions distribution monthly\n",
    "\n",
    "# convert 'Creation Time' column to datetime type\n",
    "cleaned_submissions['Creation Time'] = pd.to_datetime(cleaned_submissions['Creation Time'], format='%Y-%m-%d %H:%M:%S') # specify datetime format\n",
    "\n",
    "# extract 'Year', 'Month', 'Day' information\n",
    "cleaned_submissions['Year'] = cleaned_submissions['Creation Time'].dt.year\n",
    "cleaned_submissions['Month'] = cleaned_submissions['Creation Time'].dt.month\n",
    "cleaned_submissions['Day'] = cleaned_submissions['Creation Time'].dt.day\n",
    "\n",
    "# count monthly submissions\n",
    "subs_per_month = cleaned_submissions.groupby(['Year', 'Month']).size().reset_index(name='Post Count')\n",
    "\n",
    "# plot the monthly submissions\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(subs_per_month['Year'].astype(str) + '-' + subs_per_month['Month'].astype(str), subs_per_month['Post Count'], color='green')\n",
    "plt.ylabel('Number of Post', fontsize=18)\n",
    "plt.xticks(rotation=45, fontsize=11)\n",
    "plt.yticks(rotation=90, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53111a27-d46f-4f96-bb80-7b0686778aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the submissions for VADER analysis\n",
    "vader_submissions = pd.read_csv('/Users/Desktop/Reddit data/preprocessed_vader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371543fa-3adb-4737-86f3-de7f0a92781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77403e7a-7b4d-42f4-9e80-9bd0675c3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the VADER analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment on each post\n",
    "def analyze_vader_sentiment(text):\n",
    "    text = str(text) # convert text to string type\n",
    "    scores = sid.polarity_scores(text) # get sentiment score\n",
    "    # determine the emotion category based on the comprehensive score\n",
    "    if scores['compound'] > 0.15:\n",
    "        sentiment = 'positive' # if compound scores > 0.15 then positive sentiment\n",
    "    elif scores['compound'] < -0.15: # if compound scores < -0.14 then negative sentiment\n",
    "        sentiment = 'negative'\n",
    "    elif scores['compound'] >= -0.15 and scores['compound'] <= 0.15:\n",
    "        sentiment = 'neutral' # otherwise netural sentiment for -0.15< compound < 0.15\n",
    "    # return the sentiment category and score\n",
    "    return sentiment, scores['pos'], scores['neg'], scores['neu'], scores['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f7272-5ee7-4af9-bedb-52bb7f68bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to each post\n",
    "vader_submissions['Sentiment'], vader_submissions['Positive'], vader_submissions['Negative'], vader_submissions['Neutral'], vader_submissions['Compound_Score'] = zip(*vader_submissions['Selftext'].apply(analyze_vader_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ce0c8-9526-4866-b953-8ed33d82d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a0b4e-6f26-418e-be04-d7191f4a601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_submissions.to_csv('/Users/Desktop/Reddit data/vader_submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aca59a-d05b-4f5c-a8d1-d8500ff66568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Vader analysis to the example post\n",
    "post_text = \"I'm proud because in the past this was always the start of a pattern: If I missed gym once I missed it a second time because the perfect week was ruined anyways. Then I often would not go back to the gym at all. But not this time. I just called my gym buddy and told him to meet two days later at the gym so we could work out together. I found it really easy to stay consistent when there is someone who expects me to show up. But there is more success! My gym buddy who I relied on in the first weeks worked day shifts for 2 weeks straight now, so he could not work out when I did. But I still got my workout done, alone, with my own willpower! I also sticked to my meal plan and lost some pounds because the success in the gym motivated me.\"\n",
    "\n",
    "sentiment, pos_score, neg_score, neu_score, compound_score = analyze_vader_sentiment(post_text)\n",
    "\n",
    "# print the output\n",
    "print(f\"{post_text}\")\n",
    "print()\n",
    "print(f\"Sentiment: {sentiment}, Positive Score: {pos_score}, Negative Score: {neg_score}, Neutral Score: {neu_score}, Compound Score: {compound_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d0af0-6578-419c-a2c5-25fd1a973cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of negative, positive, and neutral posts in each time period\n",
    "sentiment_counts = vader_submissions.groupby(['Period', 'Sentiment']).size()#.unstack(fill_value=0)\n",
    "\n",
    "# Filter the data for pre-pandemic and during-pandemic periods\n",
    "pre_pandemic_posts = vader_submissions[vader_submissions['Period'] == 'pre_pandemic']\n",
    "dur_pandemic_posts = vader_submissions[vader_submissions['Period'] == 'dur_pandemic']\n",
    "\n",
    "# Count the number of positive, negative, and neutral posts in each period\n",
    "sentiment_counts_pre = pre_pandemic_posts['Sentiment'].value_counts()\n",
    "sentiment_counts_dur = dur_pandemic_posts['Sentiment'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each sentiment category within each period\n",
    "sentiment_percentages_pre = sentiment_counts_pre / sentiment_counts_pre.sum() * 100\n",
    "sentiment_percentages_dur = sentiment_counts_dur / sentiment_counts_dur.sum() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0e633-a0cb-4b64-bef2-dcef20f5f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dded3a6-bea7-4009-bc4d-cd15070eaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sentiment_percentages_pre, sentiment_percentages_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b9775-f1d0-4139-a6f2-8f0a6143e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-square test\n",
    "data = np.array([[13294, 4572],\n",
    "                 [11455, 5619]])\n",
    "chi2, p, dof, ex = chi2_contingency(data)\n",
    "print(f\"Chi2 Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
