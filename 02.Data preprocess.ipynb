{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b86f878-f80d-4584-bddd-334c13d7bdc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95aba65-debe-47b7-99df-57c1bf48be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARY\n",
    "# For decompressing and processing data\n",
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import logging.handlers\n",
    "\n",
    "# For scraping Reddit submissions\n",
    "!pip install praw\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# For data cleaning and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Import NLTK for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Import the sentiment analysis tool\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Install and load the SpaCy package\n",
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "# Download and install the SpaCy English language model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# For topic modeling\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# For temporal analysis visualization\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# For date formatting in charts\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# For interactive visualization\n",
    "import plotly.express as px\n",
    "\n",
    "import ast\n",
    "\n",
    "# For statistic test \n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f32e4-1442-495d-9cec-799f7a10eb39",
   "metadata": {},
   "source": [
    "## 2.Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590bb50d-dcc0-4aa1-87b5-925d46e4dbd9",
   "metadata": {},
   "source": [
    "This section of the code includes:\n",
    "\n",
    "- a. Clean the submission’s dataset.\n",
    "- b. Preprocess the submission’s dataset..\n",
    "- c. Save the preprocessed submissions for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcd5b4-0605-4296-8157-1f591bafc424",
   "metadata": {},
   "source": [
    "### a. Clean the submission’s dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e201a8-4cda-4b7f-93a0-b2d34e6c1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_submissions = pd.read_csv('/Users/Desktop/Reddit data/cleaned_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18713090-39a5-46ee-8fdf-1396d0486ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VADER function\n",
    "def clean_vader(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'&#x200B;|\\bx\\s*b\\b', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca18b8d-da70-4b37-865e-a8920c873f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LDA and NRC-EIL function\n",
    "def clean_lda_nrceil(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'&#x200B;|\\bx\\s*b\\b', '', text)\n",
    "    text = re.sub(r'\\bdon t\\b\", \"do not', '', text)\n",
    "    text = re.sub(r'\\bdidn t\\b\", \"did not', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17626cd0-c170-4cac-976e-d7f214b67203",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_submissions['clean_vader'] = cleaned_submissions['Selftext'].apply(clean_vader)\n",
    "cleaned_submissions['clean_lda_nrceil'] = cleaned_submissions['Selftext'].apply(clean_lda_nrceil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f129e7-2eb6-4de4-9b0f-e0691df1c162",
   "metadata": {},
   "source": [
    "### b. Preprocess the submission’s dataset.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fc881-42a7-465e-980f-f46b5fe6e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic modeling (LDA)\n",
    "\n",
    "# Step 1: Tokenize the words\n",
    "cleaned_submissions['tokenized_lda'] = [\n",
    "    word_tokenize(text) for text in cleaned_submissions['clean_lda_nrceil']\n",
    "]\n",
    "\n",
    "# Step 2: Lemmatization\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "def lemmatize_text(tokens, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "cleaned_submissions['lemmatized_lda_texts'] = [\n",
    "    lemmatize_text(words) for words in cleaned_submissions['tokenized_lda']\n",
    "]\n",
    "\n",
    "# Step 3: Remove customed and stardard stop words \n",
    "stop_words = set(stopwords.words('english')) # load stop words\n",
    "cleaned_submissions['lemmatized_lda_texts'] = [\n",
    "    [word for word in words if word.lower() not in stop_words] for words in cleaned_submissions['lemmatized_lda_texts']\n",
    "]\n",
    "\n",
    "# Step 4: Remove the word with less than 3\n",
    "cleaned_submissions['preprocessed_lda'] = [\n",
    "    ' '.join([word for word in words if len(word) >= 3]) for words in cleaned_submissions['lemmatized_lda_texts']]\n",
    "\n",
    "# Print the LDA result\n",
    "print(cleaned_submissions[['preprocessed_lda']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f225d8d-7bc6-46e4-aef2-90669dafb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment analysis (VADER)\n",
    "\n",
    "# Step 1: Tokenize the words\n",
    "cleaned_submissions['tokenized_vader'] = [\n",
    "    word_tokenize(text) for text in cleaned_submissions['clean_vader']\n",
    "]\n",
    "\n",
    "# Step 2: Remove stop words\n",
    "stop_words = set(stopwords.words('english')) # load stop words\n",
    "cleaned_submissions['vader_texts'] = [\n",
    "    [word for word in tokens if word not in stop_words] for tokens in cleaned_submissions['tokenized_vader']\n",
    "]\n",
    "\n",
    "# Step 3: Convert the list of words back into text.\n",
    "cleaned_submissions['preprocessed_vader'] = cleaned_submissions['vader_texts'].apply(lambda words: ' '.join(words))\n",
    "\n",
    "# Print the LDA result\n",
    "print(cleaned_submissions[['preprocessed_vader']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6ffe8-342b-4dbe-a570-629f9aa45cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment analysis (NRC-EIL)\n",
    "\n",
    "# Step 1: Tokenize the words\n",
    "cleaned_submissions['tokenized_nrc'] = [\n",
    "    word_tokenize(text) for text in cleaned_submissions['clean_lda_nrceil']\n",
    "]\n",
    "\n",
    "# Step 2: Lemmatization\n",
    "def lemmatize_text(tokens):\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "cleaned_submissions['lemmatized_nrc_texts'] = [\n",
    "    lemmatize_text(words) for words in cleaned_submissions['tokenized_nrc']\n",
    "]\n",
    "\n",
    "# Step 2: Remove stop words\n",
    "stop_words = set(stopwords.words('english')) # load stop words\n",
    "cleaned_submissions['lemmatized_nrc_texts'] = [\n",
    "    [word for word in words if word.lower() not in stop_words] for words in cleaned_submissions['lemmatized_nrc_texts']\n",
    "]\n",
    "\n",
    "# Step 3: Convert the list of words back into text\n",
    "cleaned_submissions['preprocessed_nrc'] = cleaned_submissions['lemmatized_nrc_texts'].apply(lambda words: ' '.join(words))\n",
    "\n",
    "# Print the LDA result\n",
    "print(cleaned_submissions[['preprocessed_nrc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c26542-aa08-4c67-822e-a17de94003d1",
   "metadata": {},
   "source": [
    "### c. Save the preprocessed submissions for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375baf11-2b70-42ca-a473-4292b9c1658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the preprocessed submissions for LDA to data frame\n",
    "cleaned_submissions['Selftext'] = cleaned_submissions['preprocessed_lda']\n",
    "preprocessed_lda = cleaned_submissions[['Creation Time', 'Selftext', 'Score', 'Number of Comments', 'Period']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4c81a-625d-420f-8342-ce9c28220e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the preprocessed submissions for VADER to data frame\n",
    "cleaned_submissions['Selftext'] = cleaned_submissions['preprocessed_vader']\n",
    "preprocesseds_vader = cleaned_submissions[['Creation Time', 'Selftext', 'Score', 'Number of Comments', 'Period']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee178136-e1e2-4d5d-b7af-0d54d4c80a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the preprocessed submissions for NRC-EIL to data frame\n",
    "cleaned_submissions['Selftext'] = cleaned_submissions['preprocessed_nrc']\n",
    "preprocessed_nrc = cleaned_submissions[['Creation Time', 'Selftext', 'Score', 'Number of Comments', 'Period']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5f9f7-bb6e-4860-96ec-a131e114b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find preprocess value with any NaN values\n",
    "na_lda = preprocessed_lda[preprocessed_lda.isna().any(axis=1)]\n",
    "print(na_lda)\n",
    "\n",
    "na_vader = preprocessed_vader[preprocessed_vader.isna().any(axis=1)]\n",
    "print(na_vader)\n",
    "\n",
    "na_nrc = preprocessed_nrc[preprocessed_nrc.isna().any(axis=1)]\n",
    "print(na_nrc)\n",
    "\n",
    "# posts with 5696,13334,31352 ids has NAN values in LDA and NRC preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664faa41-6e4a-441f-b919-b60469ed325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this three posts with NaN in the 'Selftext' column\n",
    "indices_to_drop = [5696, 13334, 31352] # 35381 submissions left in the datasets\n",
    "preprocessed_lda = preprocessed_lda.drop(indices_to_drop)\n",
    "preprocessed_vader = preprocessed_vader.drop(indices_to_drop)\n",
    "preprocessed_nrc = preprocessed_nrc.drop(indices_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95396fc-e69d-43c6-9355-6adee6170f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "preprocessed_lda = preprocessed_lda.reset_index(drop=True)\n",
    "preprocessed_vader = preprocessed_vader.reset_index(drop=True)\n",
    "preprocessed_nrc = preprocessed_nrc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41cc42-eddb-4083-b54a-347df14bd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed dataset\n",
    "preprocessed_lda.to_csv('/Users/Desktop/Reddit data/preprocessed_lda.csv', index=False)\n",
    "preprocessed_vader.to_csv('/Users/Desktop/Reddit data/preprocessed_vader.csv', index=False)\n",
    "preprocessed_nrc.to_csv('/Users/Desktop/Reddit data/preprocessed_nrc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
